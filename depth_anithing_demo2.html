<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>3Dパララックス・ディスプレイスメント効果</title>
  <style>
    body { margin: 0; overflow: hidden; background: #111; }
    #controls {
      position: absolute; top: 10px; left: 10px; z-index: 10;
      background: rgba(255,255,255,0.8); padding: 10px; border-radius: 4px;
      font-family: Arial, sans-serif;
    }
    #loadingOverlay {
      position: fixed;
      top: 0; left: 0;
      width: 100vw; height: 100vh;
      background: rgba(0, 0, 0, 0.8);
      color: white;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 24px;
      z-index: 999;
      display: none;
    }
  </style>
</head>
<body>
  <div id="controls">
    <input type="file" id="imageInput" accept="image/*">
    <p>画像を選択してください</p>
  </div>
  <div id="loadingOverlay">処理中...</div>
  <div id="threeContainer"></div>

  <!-- Three.js の読み込み -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r134/three.min.js"></script>
  <!-- ONNX Runtime Web の読み込み -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.18.0/dist/ort.wasm.min.js"></script>

  <script>
    // グローバル変数
    let scene, camera, renderer, mesh;
    let originalTexture, depthTexture;
    const container = document.getElementById("threeContainer");
    const loadingOverlay = document.getElementById("loadingOverlay");
    const imageInput = document.getElementById("imageInput");
    const size = 512; // テクスチャ用のキャンバスサイズ（正方形・2の累乗推奨）

    // オフスクリーンキャンバス（オリジナル画像用）
    const offscreenCanvas = document.createElement("canvas");
    offscreenCanvas.width = size;
    offscreenCanvas.height = size;
    const offscreenCtx = offscreenCanvas.getContext("2d");

    // --- ONNX 推論関連 ---
    // 画像データを前処理して float32 配列に変換
    function preprocess(imageData, width, height) {
      const floatArr = new Float32Array(width * height * 3);
      let j = 0;
      for (let i = 0; i < imageData.data.length; i += 4) {
        floatArr[j] = imageData.data[i] / 255;
        floatArr[j + width * height] = imageData.data[i + 1] / 255;
        floatArr[j + 2 * width * height] = imageData.data[i + 2] / 255;
        j++;
      }
      return floatArr;
    }
    // 推論結果から深度マップのキャンバスを作成
    function postprocess(tensor) {
      const [_, height, width] = tensor.dims;
      const canvas = document.createElement("canvas");
      canvas.width = width;
      canvas.height = height;
      const ctx = canvas.getContext("2d");
      const imageData = ctx.createImageData(width, height);
      const data = imageData.data;
      const tensorData = tensor.data;
      let minDepth = Infinity, maxDepth = -Infinity;
      for (let i = 0; i < tensorData.length; i++) {
        if (tensorData[i] < minDepth) minDepth = tensorData[i];
        if (tensorData[i] > maxDepth) maxDepth = tensorData[i];
      }
      for (let i = 0; i < tensorData.length; i++) {
        let normalized = (tensorData[i] - minDepth) / (maxDepth - minDepth);
        const value = normalized * 255;
        const idx = i * 4;
        data[idx] = data[idx + 1] = data[idx + 2] = value;
        data[idx + 3] = 255;
      }
      ctx.putImageData(imageData, 0, 0);
      return canvas;
    }

    // --- Three.js 関連 ---
    // Three.js のシーン初期化
    function initThree(originalTexture, depthTexture) {
      scene = new THREE.Scene();
      const aspect = window.innerWidth / window.innerHeight;
      camera = new THREE.PerspectiveCamera(40, aspect, 0.1, 1000);
      camera.position.z = 2.5;  // 初期ズーム

      renderer = new THREE.WebGLRenderer({ antialias: true });
      renderer.setSize(window.innerWidth, window.innerHeight);
      container.innerHTML = "";
      container.appendChild(renderer.domElement);

      // 平面ジオメトリ（分割数を多めにしてスムーズな変形に）
      const geometry = new THREE.PlaneGeometry(1.6, 1.6, 256, 256);
      // 材質にオリジナル画像とディスプレイスメントマップを指定
      const material = new THREE.MeshStandardMaterial({
        map: originalTexture,
        displacementMap: depthTexture,
        displacementScale: 0.2,
      });
      mesh = new THREE.Mesh(geometry, material);
      scene.add(mesh);

      // 環境光を追加
      const ambientLight = new THREE.AmbientLight(0xffffff, 1.0);
      scene.add(ambientLight);

      // リサイズ対応
      window.addEventListener("resize", onWindowResize, false);
      animate();
    }
    function onWindowResize() {
      camera.aspect = window.innerWidth / window.innerHeight;
      camera.updateProjectionMatrix();
      renderer.setSize(window.innerWidth, window.innerHeight);
    }
    function animate() {
      requestAnimationFrame(animate);
      renderer.render(scene, camera);
    }

    // --- マウス操作 ---
    // マウス移動でカメラ位置を微調整（パララックス効果）
    document.addEventListener("mousemove", (e) => {
      const mouseX = (e.clientX / window.innerWidth) * 2 - 1;
      const mouseY = -(e.clientY / window.innerHeight) * 2 + 1;
      // カメラ位置に少しずらしを加える
      camera.position.x = mouseX * 0.2;
      camera.position.y = mouseY * 0.2;
      camera.lookAt(scene.position);
    });
    // マウスホイールでズーム（カメラの前後移動）
    document.addEventListener("wheel", (e) => {
      camera.position.z += e.deltaY * 0.001;
      camera.position.z = THREE.MathUtils.clamp(camera.position.z, 1.5, 5.0);
    });

    // --- 画像処理＆シーン初期化 ---
    async function processImage(file) {
      loadingOverlay.style.display = "flex";
      const img = new Image();
      img.src = URL.createObjectURL(file);
      await new Promise(resolve => { img.onload = resolve; });
      // オフスクリーンキャンバスに画像を描画
      offscreenCtx.drawImage(img, 0, 0, size, size);

      // ONNX に入力するための画像データ取得
      const imageData = offscreenCtx.getImageData(0, 0, size, size);

      // ONNX モデルの読み込み
      const session = await ort.InferenceSession.create(
        'https://cdn.glitch.me/0f5359e2-6022-421b-88f7-13e276d0fb33/depthanythingv2-vits-dynamic-quant.onnx',
        { executionProviders: ['wasm'] }
      );
      const inputTensor = new ort.Tensor("float32", preprocess(imageData, size, size), [1, 3, size, size]);
      const feeds = { image: inputTensor };
      const results = await session.run(feeds);
      const depthTensor = results.depth;
      // 推論結果から深度マップキャンバスを作成
      const depthCanvas = postprocess(depthTensor);

      // Three.js 用テクスチャを作成
      originalTexture = new THREE.CanvasTexture(offscreenCanvas);
      depthTexture = new THREE.CanvasTexture(depthCanvas);
      depthTexture.minFilter = THREE.LinearFilter;
      depthTexture.magFilter = THREE.LinearFilter;

      // Three.js シーンを初期化
      initThree(originalTexture, depthTexture);
      loadingOverlay.style.display = "none";
    }

    imageInput.addEventListener("change", (e) => {
      const file = e.target.files[0];
      if (file) {
        processImage(file);
      }
    });
  </script>
</body>
</html>
